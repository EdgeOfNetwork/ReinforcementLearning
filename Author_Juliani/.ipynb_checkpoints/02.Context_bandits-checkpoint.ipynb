{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d8a5478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\qhrrl\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import tf_slim as slim\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b32ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf_slim\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "     -------------------------------------- 352.1/352.1 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py>=0.2.2 in c:\\users\\qhrrl\\anaconda3\\lib\\site-packages (from tf_slim) (1.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\qhrrl\\anaconda3\\lib\\site-packages (from absl-py>=0.2.2->tf_slim) (1.16.0)\n",
      "Installing collected packages: tf_slim\n",
      "Successfully installed tf_slim-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install tf_slim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39cfe0c",
   "metadata": {},
   "source": [
    "# Context 즉 state를 통해 Agent가 State 값을 통해 양의 보상을 가장 자주 주는 손잡이를 선택하는 방법을 학습하게 함이 목표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "740c1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContexualBandit:\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        self.bandits = np.array([[0.2, 0, -0.0, -5],\n",
    "                                [0.1, -5, 1, 0.25],\n",
    "                               [-5, 5, 5, 5]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def get_bandit(self):\n",
    "        self.state = np.random.randint(0,len(self.bandits))\n",
    "        return self.state\n",
    "    \n",
    "    def pull_arm(self, action):\n",
    "        bandit = self.bandits[self.state, action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f709c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, lr, s_size, a_size):\n",
    "        self.state_in = tf.placeholder(shape=[1], dtype = tf.int32)\n",
    "        state_in_OH   = slim.one_hot_encoding(self.state_in, s_size)\n",
    "        output = slim.fully_connected(state_in_OH, a_size,\\\n",
    "                                     biases_initializer = None,\\\n",
    "                                     activation_fn = tf.nn.sigmoid,\\\n",
    "                                     weights_initializer = tf.ones_initializer())\n",
    "        self.output = tf.reshape(output, [-1])\n",
    "        self.chosen_action = tf.argmax(self.output, 0)\n",
    "        \n",
    "        self.reward_holder = tf.placeholder(shape=[1], dtype = tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[1], dtype = tf.int32)\n",
    "        self.responsible_weight = tf.slice(self.output, self.action_holder, [1])\n",
    "        self.loss = -(tf.log(self.responsible_weight) * self.reward_holder)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = lr)\n",
    "        self.update = optimizer.minimize(self.loss)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b9df931",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "context_bandit = ContexualBandit()\n",
    "agent = Agent(lr = 0.001, s_size = context_bandit.num_bandits,\n",
    "              a_size = context_bandit.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec2e3607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward 4 each of the 3bandits : [0.   0.   0.25]\n",
      "Mean reward 4 each of the 3bandits : [35.75 33.75 36.75]\n",
      "Mean reward 4 each of the 3bandits : [70.5  74.5  71.75]\n",
      "Mean reward 4 each of the 3bandits : [108.5  114.25 108.5 ]\n",
      "Mean reward 4 each of the 3bandits : [142.   151.5  148.25]\n",
      "Mean reward 4 each of the 3bandits : [178.25 186.   183.5 ]\n",
      "Mean reward 4 each of the 3bandits : [218.   223.75 214.5 ]\n",
      "Mean reward 4 each of the 3bandits : [260.25 257.75 247.75]\n",
      "Mean reward 4 each of the 3bandits : [295.25 297.   285.5 ]\n",
      "Mean reward 4 each of the 3bandits : [335.25 338.75 321.75]\n",
      "Mean reward 4 each of the 3bandits : [374.75 377.   356.  ]\n",
      "Mean reward 4 each of the 3bandits : [411.75 412.   394.5 ]\n",
      "Mean reward 4 each of the 3bandits : [447.25 450.25 434.25]\n",
      "Mean reward 4 each of the 3bandits : [485.25 487.5  470.5 ]\n",
      "Mean reward 4 each of the 3bandits : [524.75 526.25 503.75]\n",
      "Mean reward 4 each of the 3bandits : [562.   567.75 535.5 ]\n",
      "Mean reward 4 each of the 3bandits : [599.25 604.25 567.75]\n",
      "Mean reward 4 each of the 3bandits : [639.25 641.   600.5 ]\n",
      "Mean reward 4 each of the 3bandits : [677.75 676.75 637.25]\n",
      "Mean reward 4 each of the 3bandits : [713.   714.   673.75]\n",
      "The Agent think action 4 for bandit 1 is the most promising\n",
      "... and it was right!\n",
      "The Agent think action 2 for bandit 2 is the most promising\n",
      "... and it was right!\n",
      "The Agent think action 1 for bandit 3 is the most promising\n",
      "... and it was right!\n"
     ]
    }
   ],
   "source": [
    "weights = tf.trainable_variables()[0]\n",
    "\n",
    "total_episodes = 10000\n",
    "\n",
    "#밴딧에 대한 점수판을 0으로 설정\n",
    "total_reward = np.zeros([context_bandit.num_bandits, context_bandit.num_actions])\n",
    "\n",
    "e = 0.1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#텐서플로 그래프 론칭\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        s = context_bandit.get_bandit() #환경으로부터 state 가져오기\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(context_bandit.num_actions)\n",
    "        else:\n",
    "            action = sess.run(agent.chosen_action, feed_dict={agent.state_in :[s]})\n",
    "            \n",
    "        reward = context_bandit.pull_arm(action)\n",
    "        \n",
    "        feed_dict = {agent.reward_holder : [reward],\\\n",
    "                    agent.action_holder : [action],\n",
    "                    agent.state_in : [s]}\n",
    "        _, ww = sess.run([agent.update, weights], feed_dict = feed_dict)\n",
    "        \n",
    "        total_reward[s, action] += reward\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(\"Mean reward 4 each of the \" + str(context_bandit.num_bandits) +\n",
    "                 \"bandits : \"+ str(np.mean(total_reward, axis = 1)))\n",
    "        i += 1\n",
    "        \n",
    "for a in range(context_bandit.num_bandits):\n",
    "    print(\"The Agent think action \" + str(np.argmax(ww[a]) + 1) + \" for bandit \" +\n",
    "         str(a + 1) + \" is the most promising\")\n",
    "    if np.argmax(ww[a]) == np.argmin(context_bandit.bandits[a]):\n",
    "        print(\"... and it was right!\")\n",
    "    else:\n",
    "        print(\"... and it was wrong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf5449e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d75a84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
